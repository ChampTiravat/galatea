#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
The basic idea is to try to learn a set of transformations 
\begin_inset Formula $\left\{ t_{1}(w),t_{2}(w),\dots t_{N_{T}}(w)\right\} $
\end_inset

 such that if 
\begin_inset Formula $W_{i}$
\end_inset

 is a useful weight vector, then 
\begin_inset Formula $t_{j}(W_{i})$
\end_inset

 or something similar to it is probably also a useful weight vector.
\end_layout

\begin_layout Standard
There are a few advantages of this:
\end_layout

\begin_layout Itemize
If two weight vectors are known to be related by some tranformations, then
 they can share statistical strength
\end_layout

\begin_layout Itemize
Learning might possible be accelerated, because once we know a few tranformation
s and a few weight vectors this can start proposing new weight vectors,
 rather than waiting for gradient descent to blindly find all of them
\end_layout

\begin_layout Itemize
Knowing how weight vectors are related by transformations can inform a pooling
 strategy
\end_layout

\begin_layout Standard
There are two different basic ways to employ this strategy that I can think
 of off the top of my head:
\end_layout

\begin_layout Itemize
Parameterize the weight matrix 
\begin_inset Formula $W$
\end_inset

 such that some of its columns are specified as being transformed versions
 of its other columns
\end_layout

\begin_layout Itemize
Put a prior on 
\begin_inset Formula $W$
\end_inset

 such that it columns are more likely to be transformed versions of each
 other
\end_layout

\begin_layout Standard
Currently I lean toward the second strategy.
 The first strategy requires that all of 
\begin_inset Formula $W$
\end_inset

 be recomputed after each learning update.
 In the second, the prior will be very expensive to compute, but we can
 stochastically apply just a subset of its terms during each gradient step.
\end_layout

\begin_layout Standard
There are two ways of getting good statistical strength out of this approach:
\end_layout

\begin_layout Itemize
Share the transformations across several base templates.
 This way the transformation must be useful in many contexts.
\end_layout

\begin_layout Itemize
Repeatedly compose the transformation.
 For example, have one feature be 
\begin_inset Formula $w$
\end_inset

, another 
\begin_inset Formula $t(w)$
\end_inset

, another 
\begin_inset Formula $t(t(w))$
\end_inset

, etc.
 This is because many transformations like translation, in-plane rotation,
 etc.
 have the property that tranforming by amount 
\begin_inset Formula $\alpha$
\end_inset

 is the same as tranforming by amount 
\begin_inset Formula $\alpha/2$
\end_inset

 twice.
 We can exploit that property to get a really good model of a small amount
 of transformation rather than trying to learn each amount of transformation
 separately.
\end_layout

\begin_layout Standard
Things remaining to be resolved:
\end_layout

\begin_layout Itemize
Should the prior be directed or undirected?
\end_layout

\begin_layout Itemize
It would probably be better rather than having 
\begin_inset Formula $w_{i}$
\end_inset

 be directly a transformed version of 
\begin_inset Formula $w_{j}$
\end_inset

, to have some underlying feature description 
\begin_inset Formula $f_{i}$
\end_inset

and 
\begin_inset Formula $f_{j}$
\end_inset

 which are transformed versions of each other, then have 
\begin_inset Formula $w_{i}$
\end_inset

and 
\begin_inset Formula $w_{j}$
\end_inset

 be renderings of these descriptions.
 This allows you to implement things like rotations/translations of underlying
 images that are larger than the image being modeled.
\end_layout

\begin_layout Itemize
Work out the actual math for the stochastic prior updates
\end_layout

\begin_layout Section
Probabilistic model
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\log P(W,X\mid F,T)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\log P(X\mid W,F,T)+\log P(W\mid F,T)+\log P(F\mid T)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\log P(X\mid W)+\log P(W\mid F)+\log P(F\mid T)
\]

\end_inset


\end_layout

\begin_layout Subsection
\begin_inset Formula $\log P(X\mid W)$
\end_inset


\end_layout

\begin_layout Standard
This is just any standard model based on 
\begin_inset Formula $W$
\end_inset

 out there.
 Like an RBM or something.
\end_layout

\begin_layout Subsection
\begin_inset Formula $\log P(W\mid F)$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\log P(W\mid F)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\log\Pi_{i}P(w_{i}\mid f_{i})
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\log\Pi_{i}\mathcal{N}(w_{i}\mid f_{i}[0:N],\alpha^{(i)-1})
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\sum_{i}\log\mathcal{N}(w_{i}\mid f_{i}[0:N],\alpha^{(i)-1})
\]

\end_inset


\end_layout

\begin_layout Subsection
\begin_inset Formula $\log P(F\mid T)$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\log P(F\mid T)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\sum_{i}\log\left[\frac{1}{|Pa(f_{i})|}\sum_{j}n(f_{i},T^{(j)}Pa_{j}(f_{i}),\beta^{(i,j)})\right]
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\sum_{i}\log\left[\frac{1}{|Pa(f_{i})|}\sum_{j}n(f_{i},T^{(j)}Pa_{j}(f_{i}),\beta^{(i,j)})\right]
\]

\end_inset


\end_layout

\end_body
\end_document
