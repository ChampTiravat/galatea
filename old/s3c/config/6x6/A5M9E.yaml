#like A5M9E (everything but W frozen, in order to debug decreasing EM functional)
#decreasing EM functional turned out to be entirely the fault of W
#but is this due to the W update being wrong, since reprojecting to unit sphere is not
#a real solution method, or is it due to the E step getting less accurate as the interactions
#become more complicated?
#here I try adding more fixed point updates
!obj:pylearn2.scripts.train.Train {
    "dataset": !pkl: "${GOODFELI_TMP}/cifar10_preprocessed_train_2M_6x6.pkl",
    "model": !obj:galatea.s3c.s3c.S3C {
               "nvis" : 108,
               "nhid" : 1600,
               "init_bias_hid" : -4.,
               "min_bias_hid" : -4.,
               "max_bias_hid" : -4.,
               "irange"  : .02,
               "init_B"  : 3.,
               "min_B"   : 3.,
               "max_B"   : 3.,
               "tied_B" :  1,
               "init_alpha" : 1.,
               "min_alpha" : 1.,
               "max_alpha" : 1.,
               "init_mu" : 0.,
               "min_mu"  : 0.,
               "max_mu"  : 0.,
               "constrain_W_norm" : 1.,
               #"recycle_q" : 1000,
               "print_interval" : 1,
               "monitor_functional" : 1,
               "monitor_norms" : 1,
               #"monitor_stats" : [ 'mean_h', 'mean_hs', 'mean_sq_s', 'mean_sq_hs' ],
               "e_step" : !obj:galatea.s3c.s3c.VHS_E_Step {
                        "h_new_coeff_schedule" : [ .1, .1, .1, .1, .1, .1, .1, .1, .2, .2, .2, .3, .3, .3, .4, .4, .4, .4, .4, .4, .4, .4, .4, .4, .4, .4 ],
                        "s_new_coeff_schedule" : [ .7, .1, .1, .1, .1, .1, .1, .1, .1, .1, .1, .1, .1, .1, .1, .1, .1, .1, .1, .1, .1, .1, .1, .1, .1, .1 ],
                        "clip_reflections" : 1,
                        "monitor_em_functional" : 1
               },
               "new_stat_coeff" : 1.,
               #"learn_after" : 1000,
               "m_step"     : !obj:galatea.s3c.s3c.VHS_Solve_M_Step {
                        "new_coeff" : .1
                        #"learning_rate" : 1e-3
               },
               "mu_eps" : 0.,
        },
    "algorithm": !obj:pylearn2.training_algorithms.default.DefaultTrainingAlgorithm {
               "batch_size" : 2000,
               "batches_per_iter" : 10,
               "monitoring_batches" : 1,
               "monitoring_dataset" : !pkl: "${GOODFELI_TMP}/cifar10_preprocessed_train_2M_6x6.pkl",
        },
    "save_path": "${PYLEARN2_TRAIN_FILE_NAME}.pkl"
}

