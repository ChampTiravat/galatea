#DJ seems to have mostly solved the inference problem
#the KL error went to at most .06
#hopefully this is a small enough amount not to ruin learning
#though I am kind of worried about the model "learning to make
#inference right"
#here I start trying to get good learning results
!obj:pylearn2.scripts.train.Train {
        #save_path: "/kermit",
        save_path: "${EXPDIR}/pddbm_mnist/${PYLEARN2_TRAIN_FILE_STEM}.pkl",
    dataset: &src !obj:pylearn2.datasets.mnist.MNIST {
                        "which_set" : "train",
                        "center" : 0
                     },
    model: !obj:galatea.pddbm.pddbm.PDDBM {
        learning_rate : .001,
        h_bias_src: "s3c",
        freeze_s3c_params: 1,
        monitor_ranges: 1,
        dbm: !obj:pylearn2.models.dbm.DBM {
                negative_chains : 100,
                monitor_params: 1,
                rbms : [ !obj:pylearn2.models.rbm.RBM {
                                nvis: 529,
                                nhid: 529,
                                irange: 1.,
                                init_bias_hid: -1.5
                } ],
        },
        s3c: !pkl: "${GOODFELI_TMP}/s3c_mnist/B1E.pkl",
        inference_procedure : !obj:galatea.pddbm.pddbm.InferenceProcedure {
               schedule : [
#2
 ['s', .2], ['h', .2], ['g',0],
#5
 ['h', .2], ['s', .2], ['h', .2], 
#8
 ['g',0], ['h', .2], ['s', .1],
#11
 ['h',0.1],['g', 0],['h', .1],
#14
 ['s',.1], ['h',.1], ['g',0],
#17
 ['h',0.1], ['s',.1],['h', .1],
 #20
 ['g',0], ['h', .1],['s', .1],
 #23
 ['h',.1],['g', 0], ['h', .1],
 #26
 ['s',.1], ['h',.1], ['g',0],
 #29
 ['h',0.1], ['s',.2], ['h', .2],
 #32
 ['g',0], ['h', .2],['s', .2],
 #35
 ['h',0.2],['g', 0],['h', .2],
 #38
 ['s',.2], ['h',.2], ['g',0],
 #41
 ['h',0.3], ['s',.3],['h', .3],
 #44
 ['g',0], ['h', .3],['s', .3],
 #47
 ['h',0.4],['g', 0], ['h', .4],
 #50
 ['s',.4], ['h',.4],
#52
['g',0], ['h',0.5], ['s',.5],
#55
 ['h', .5], ['g',0], ['h',.5],
#58
['s',.6],['h',.6],['g',0],
#61
 ['h',0.6],['g', 0], ['h', .6],
 #64
 ['s',.7], ['h',.7],
#66
['g',0], ['h',0.7], ['s',.7],
#69
 ['h', .7], ['g',0], ['h',.7],
#72
['s',.7],['h',.7],['g',0],
 ['h',0.7], ['s',.7],['h', .7],
 #75
 ['g',0], ['h', .7],['s', .7],
 #78
 ['h',0.7],['g', 0], ['h', .7],
 #81
 ['s',.7], ['h',.7],
#83
['g',0], ['h',0.7], ['s',.7],
#86
 ['h', .7], ['g',0], ['h',.7],
#89
['s',.7],['h',.7],['g',0],
#92
 ['h',0.3],['g', 0], ['h', .3],
 #95
 ['s',.3], ['h',.4],
#97
['g',0], ['h',0.4], ['s',.2],
#100
 ['h', .4], ['g',0], ['h',.2],
#103
['s',.2],['h',.2],['g',0],
                            ],
                clip_reflections : 1,
                rho : 1.,
                monitor_kl : [1,-1,10],
        },
    },
    algorithm: !obj:pylearn2.training_algorithms.default.DefaultTrainingAlgorithm {
               batch_size : 100,
               "batches_per_iter" : 100,
               "monitoring_batches" : 1,
               "monitoring_dataset" : !obj:pylearn2.datasets.dense_design_matrix.from_dataset {
                        "dataset" : *src,
                        "num_examples" : 100
                }
        },
    save_freq: 1,
    callbacks: [ galatea.pddbm.batch_gradient_inference_callback.BatchGradientInferenceCallback() ]
}

