This directory contain the 6 datasets from the

Unsupervised and Transfer Learning Challenge(UTLC)

The web page is at: http://www.causality.inf.ethz.ch/home.php

Dataset 	 Domain                         nbFeat	Sparsity(%)	nb train nb valid nb test Transfer num.
AVICENNA 	 Arabic manuscripts		120	0.00		150205	 4096	  4096	  50000
HARRY 		 Human action recognition 	5000 	98.12 		69652 	 4096 	  4096 	  20000
RITA 		 Object recognition 		7200 	1.19 		111808 	 4096 	  4096 	  24000
SYLVESTER 	 Ecology 			100 	0.00 		572820 	 4096 	  4096 	  100000
TERRY 		 Text recognition 		47236 	99.84 		217034 	 4096 	  4096 	  40000
ULE (toy data) 	 Handwritten digits 		784 	80.85 		26808 	 4096 	  4096 	  10000

    	 	 	     	memory size(2)
    	 	 Nb element(1)  int16   float32 float64 original value range 	    original type
AVICENNA 	 19M		~38MB	~76MB	~150MB	0-999	       	     	    int
HARRY		 389M		~780MB	~1560MB	~3GB	tr(0-451)va(0-19)te(0-24)   int
RITA		 864M		~1728MB	~3456MB	~7GB	tr/va/te 0-230	     	    int
SYLVESTER	 58M		~116MB	~232MB	~460MB	0-999	 	     	    int
TERRY		 10368M		~21GB	~43GB	~86GB	tr(2-999)va(3-728)te(2-684) int
ULE		 27M		~55MB	~110MB	~220MB	0-255			    int
(1)(nb rows train+valid+test)*nbFeat/1000./1000
(2) if stored in an ndarray.

    	      	 memory size of sparse format.
    	 	 uint8 int16   float32 float64
HARRY		       42MB    56MB    84MB
TERRY		       98MB    131MB   196MB
ULE		 25MB		41MB   60MB

       	      stored dtype    dtype after normalization
AVICENNA      int16  	      floatX
HARRY	      int16	      floatX(float32=1.5G, float64=3G) or sparse floatX
RITA	      uint8	      floatX(float32=3.5G, float64=7G)
SYLVESTER     int16	      floatX
TERRY	      int16	      sparse floatX
ULE	      uint8	      floatX or sparse floatX

	      train set stats before normalization
	  max  min  mean   std   non-zeros
AVICENNA  999  0    514.62 6.829 
HARRY	  451  0    0.0481 0.707 7109733
RITA	  230  0    106.24 56.78 
SYLVESTER 999  0    403.81 96.43 
TERRY     999  0    0.1100 	 16298648
ULE       255  0    33.290 78.48 4027467

	      train set stats after normalization
	  max 	 min     mean    std
AVICENNA  70.928 -75.36  ~0      ~1
HARRY	  650.45 0	 0.0687  0.9992
RITA	  1.0	 0.0	 0.0208	 0.1444
SYLVESTER 6.1716 -4.187  ~0      ~1
TERRY     3.33   0	 0.00037
ULE       1      0       0.1305  0.3078


All train set have been shuffled. 

All dataset are normalized AT LOAD TIME or ON THE FLY(see bellow)!
This is to allow to use a smaller dtype when storing them.

To load them, use the code in the file pylearn.datasets.utlc.py

The datasets avicenna, harry, rita ans sylvester have been stored in
the filetensor format. They will result in ndarray in memory.
Most have been compressed with gzip except when that was not usefull.

The dataset harry, terry, ule have been stored as pickle of
scipy.sparse.csr_matrix. Terry have been stored like this because it
is very sparse and use too much memory otherwise(43GB in float32). harry would use
much memory, but if we take care that could work. I also put ule
to allow you to test a small sparse dataset in your algo.

Normalization done and note by Yoshua
-------------------------------------

avicenna: soustraire la moyenne puis diviser par l'écart-type 
          (NOTE: résultat réel, reconstruire avec erreur quadratique, unités linéaires)
ule: diviser par le max  
          (NOTE: résultat dans (0,1), reconstruire avec des sigmoides et coût cross-entropy)
harry: diviser par l'écart-type 
          (NOTE: résultat positif, reconstruire avec erreur quadratique et unités softplus)
rita: diviser par le max 
          (NOTE: résultat dans (0,1), reconstruire avec des sigmoides et coût cross-entropy)
sylvester:  soustraire la moyenne puis diviser par l'écart-type 
          (NOTE: résultat réel, reconstruire avec erreur quadratique, unités linéaires, 
	  ou bien bien scaler par rapport au max et utiliser des sigmoides)
terry: diviser par 300.

On the fly normalization
------------------------

Implemented only for ndarray format, not for the sparse format.

Now it is possible to do the normalization on the fly of minibatch.
That mean we shore the data in its original dtype that take less space,
and compute the normalization on each minibatch. So we use less memory
and can use computer with less memory.

To enable that, you must call that function:

pylearn.datasets.utlc.load_ndarray_dataset(DATASET_NAME, normaliza=False, normaliza_on_the_fly=True)

That will return Theano variable(and not ndarray). You MUST first take a subtensor of it(minibatch) before doing your computation on it.
What happen is that a new Theano optimization will move the elemwise computation after the subtensor. So you won't compute the normalization
of the full dataset.

As it return Theano variable, if you do train.shape[0], it return a Theano variable. You can as theano to compute the output. 
There is the ift6266h11.dense.logistic_sgd.get_constant(train.shape[0]) that will do it for you.

Transfer label
--------------

Currently only the ule transfer label are available.

ule train/valid/test label:
As ule is a toy dataset, we have the label of the part. They can be loaded
with the function pylearn.datasets.utlc.load_ndarray_label()

They have been stored in the filetensor and sparse format. It is a matrix with
the same number of row as the train set. It is shuffled the same way as the train set.
So the train set and the transfer label are correctly aligned.

Each columns represent a class.Not all class have a column. So for some example we 
don't know the class(the row contain only 0). We also don't know the number of class I think...