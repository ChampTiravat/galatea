#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section
Yoshua's randomized pooling idea
\end_layout

\begin_layout Standard
Say we have some criterion C(y(x,M)) where 
\begin_inset Formula $y_{i}(x,M)=\max_{j}x_{j}M_{ij}$
\end_inset

.
 
\begin_inset Formula $M$
\end_inset

 is fixed prior to each evaluation/gradient ascent step on 
\begin_inset Formula $C$
\end_inset

.
 
\begin_inset Formula $M$
\end_inset

 is generated by 
\begin_inset Formula $M_{ij}\sim\mathcal{B}(I_{ij})$
\end_inset

.
 The problem of learning to pool is then the problem of learning 
\begin_inset Formula $I$
\end_inset

.
 
\end_layout

\begin_layout Standard
We would like to compute
\begin_inset Formula 
\[
\frac{\partial}{\partial I_{ij}}\mathbb{E}[C(y(x,M))]=\frac{\partial}{\partial I_{ij}}\sum_{M}P(M)C(y(x,M))
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\sum_{M}C(y(x,M))\frac{\partial}{\partial I_{ij}}P(M)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\sum_{M}C(y(x,M))\frac{\partial}{\partial I_{ij}}\Pi_{kl}I_{kl}^{M_{kl}}(1-I_{kl})^{(1-M_{kl})}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\sum_{M}C(y(x,M))\frac{\partial}{\partial I_{ij}}\Pi_{kl}I_{kl}^{M_{kl}}(1-I_{kl})^{(1-M_{kl})}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\sum_{M}C(y(x,M))\left(\Pi_{kl\neq ij}I_{kl}^{M_{kl}}(1-I_{kl})^{(1-M_{kl})}\right)\frac{I_{ij}^{M_{ij}-1}(M_{ij}-I_{ij})}{(1-I_{ij})^{M_{ij}}}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\sum_{M}C(y(x,M))\left(\Pi_{kl\neq ij}I_{kl}^{M_{kl}}(1-I_{kl})^{(1-M_{kl})}\right)(2M_{ij}-1)
\]

\end_inset


\end_layout

\begin_layout Standard
Now we would like to implement this with a sampling approximation.
 So we re-arrange the terms inside the summation to include a factor 
\begin_inset Formula $P(M)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\sum_{M}C(y(x,M))\frac{P(M)}{I_{ij}^{M_{ij}}(1-I_{ij})^{1-M_{ij}}}(2M_{ij}-1)
\]

\end_inset


\end_layout

\begin_layout Standard
Note that 
\series bold
this only applies if 
\begin_inset Formula $I_{ij}\neq0$
\end_inset

 and 
\begin_inset Formula $I_{ij}\neq1$
\end_inset

.

\series default
 Otherwise rather than multiplying by 1 we have multiplied by 0/0.
\end_layout

\begin_layout Standard
Finally, we can replace this with samples of 
\begin_inset Formula $M$
\end_inset

 drawn from 
\begin_inset Formula $P(M)$
\end_inset

.
 Note that there are still some numerical problems even when 
\begin_inset Formula $I_{ij}$
\end_inset

is not deterministic because we may rarely divide by 
\begin_inset Formula $\epsilon$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\frac{1}{m}\sum_{k=1}^{m}C(y(x,\hat{M}^{(k)}))\frac{2\hat{M}_{ij}^{(k)}-1}{I_{ij}^{\hat{M}_{ij}^{(k)}}(1-I_{ij})^{1-\hat{M}_{ij}^{(k)}}}
\]

\end_inset


\end_layout

\begin_layout Standard
Yoshua had a further idea for avoiding the divide by 
\begin_inset Formula $\epsilon$
\end_inset

 which is described in an e-mail from him.
 I think I will start with this version though.
\end_layout

\begin_layout Standard
We can use the above formula if we constrain 
\begin_inset Formula $I_{ij}$
\end_inset

 to be 
\begin_inset Formula $\epsilon$
\end_inset

 away from deterministic.
\end_layout

\begin_layout Standard
If we wish to allow 
\begin_inset Formula $I_{ij}$
\end_inset

 to be deterministic we need an alternate approach to estimating the gradient
 when it reaches those values:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\sum_{M}C(y(x,M))\left(\Pi_{kl\neq ij}I_{kl}^{M_{kl}}(1-I_{kl})^{(1-M_{kl})}\right)(2M_{ij}-1)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\sum_{M_{-ij}}\left(\Pi_{kl\neq ij}I_{kl}^{M_{kl}}(1-I_{kl})^{(1-M_{kl})}\right)\sum_{M_{ij}}C(y(x,M))(2M_{ij}-1)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\sum_{M_{-ij}}P(M_{-ij})\sum_{M_{ij}}C(y(x,M))(2M_{ij}-1)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\sum_{M_{-ij}}P(M_{-ij})\left[C(y(x,M_{-ij},M_{ij}=1))-C(y(x,M_{-ij},M_{ij}=0))\right]
\]

\end_inset


\end_layout

\begin_layout Standard
We can approximate this using samples of 
\begin_inset Formula $M$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\frac{1}{m}\sum_{k=1}^{m}\left[C(y(x,\hat{M}_{-ij},M_{ij}=1))-C(y(x,\hat{M}_{-ij},M_{ij}=0))\right]
\]

\end_inset


\end_layout

\begin_layout Standard
This is a very intuitive gradient; it's the average difference in cost between
 using 
\begin_inset Formula $M_{ij}=1$
\end_inset

 and using 
\begin_inset Formula $M_{ij}=0$
\end_inset

, where the average is taken over random outcomes for the other mask elements.
\end_layout

\begin_layout Standard
Unfortunately evaluating the full gradient will require evaluating 
\begin_inset Formula $C$
\end_inset

 at a different point for every element of 
\begin_inset Formula $M$
\end_inset

.
 This is too expensive.
\end_layout

\begin_layout Standard
Yoshua proposes making a Taylor series expansion:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
C(y(x,\hat{M}_{-ij},M_{ij}=1-\hat{M}_{ij}))=C(y(x,\hat{M}))+(y(x,\hat{M}_{-ij},M_{ij}=1-\hat{M}_{ij})-y(x,\hat{M}))^{T}\nabla_{y}C(y(x,\hat{M}))
\]

\end_inset


\end_layout

\begin_layout Standard
We can use this to approximate the gradient with respect to 
\begin_inset Formula $I$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{1}{m}\sum_{k=1}^{m}\left[C(y(x,\hat{M})-C(y(x,\hat{M}_{-ij},M_{ij}=1-\hat{M}_{ij}))\right](2\hat{M}_{ij}-1)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\approx\frac{1}{m}\sum_{k=1}^{m}(2\hat{M}_{ij}-1)(y(x,\hat{M})-y(x,\hat{M}_{-ij},M_{ij}=1-\hat{M}_{ij}))^{T}\nabla_{y}C(y(x,\hat{M}))
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\frac{1}{m}\sum_{k=1}^{m}(2\hat{M}_{ij}-1)(y_{i}(x,\hat{M})-y_{i}(x,\hat{M}_{-ij},M_{ij}=1-\hat{M}_{ij}))\frac{\partial}{\partial y_{i}}C(y(x,\hat{M}))
\]

\end_inset


\end_layout

\begin_layout Standard
In other words, to approximate the gradient with respect to 
\begin_inset Formula $I$
\end_inset

, we multiply the gradient with respect to 
\begin_inset Formula $y$
\end_inset

 by the amount of 
\begin_inset Formula $y$
\end_inset

 that can be gained by flipping the corresponding element of 
\begin_inset Formula $M$
\end_inset

 from off to on.
\end_layout

\begin_layout Standard
Let's consider how to compute 
\begin_inset Formula $y_{i}(x,\hat{M})-y_{i}(x,\hat{M}_{-ij},M_{ij}=1-\hat{M}_{ij})$
\end_inset

.
 Let 
\begin_inset Formula $S_{ij}=\max\left\{ x_{k}\mid\hat{M}_{ik}=1\wedge k\neq j\right\} $
\end_inset

.
\end_layout

\begin_layout Standard
There are three cases:
\end_layout

\begin_layout Enumerate
\begin_inset Formula $x_{j}\leq S_{ij}$
\end_inset

 .
 In this case the inclusion or exclusion of 
\begin_inset Formula $x_{j}$
\end_inset

has no effect so the difference is 0.
\end_layout

\begin_layout Enumerate
\begin_inset Formula $x_{j}>S_{ij}$
\end_inset

 and 
\begin_inset Formula $\hat{M_{ij}}=0$
\end_inset

.
 In this case the difference is 
\begin_inset Formula $y_{i}(x,\hat{M})-x_{j}$
\end_inset

.
 This will always be negative, but it is multiplied by 
\begin_inset Formula $(2\hat{M}_{ij}-1)$
\end_inset

, which is 
\begin_inset Formula $-1$
\end_inset

 in this case.
\end_layout

\begin_layout Enumerate
\begin_inset Formula $x_{j}>S_{ij}$
\end_inset

 and 
\begin_inset Formula $\hat{M}_{ij}=1.$
\end_inset

 In this case the difference is 
\begin_inset Formula $x_{j}-S_{ij}$
\end_inset

.
 The difference is always positive, and it is multiplied by 
\begin_inset Formula $(2\hat{M}_{ij}-1)$
\end_inset

, which is 1.
\end_layout

\begin_layout Standard
The above observations suggest we can simplify the approximation of the
 gradient to:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\frac{1}{m}\sum_{k=1}^{m}\left|y_{i}(x,\hat{M})-y_{i}(x,\hat{M}_{-ij},M_{ij}=1-\hat{M}_{ij})\right|\frac{\partial}{\partial y_{i}}C(y(x,\hat{M}))
\]

\end_inset


\end_layout

\begin_layout Standard
Let's try splitting the cases a different way.
\end_layout

\begin_layout Standard
Consider the case where
\begin_inset Formula $\hat{M}_{ij}=0$
\end_inset

.
 In this case 
\begin_inset Formula $S_{ij}=y_{i}$
\end_inset

.
 So we don't need to explicitly compute 
\begin_inset Formula $S_{ij}$
\end_inset

 in this case.
 The difference is 
\begin_inset Formula $y_{i}(x,\hat{M})-x_{j}$
\end_inset

.
\end_layout

\begin_layout Standard
We can now simplify the update to:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\frac{1}{m}\sum_{k=1}^{m}\left[(1-\hat{M}_{ij})\left(x_{j}-y_{i}(x,\hat{M})\right)^{+}+\hat{M}_{ij}\left|y_{i}(x,\hat{M})-y_{i}(x,\hat{M}_{-ij},M_{ij}=0)\right|\right]\frac{\partial}{\partial y_{i}}C(y(x,\hat{M}))
\]

\end_inset


\end_layout

\begin_layout Standard
where the superscript 
\begin_inset Formula $+$
\end_inset

indicates taking the positive part.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\frac{1}{m}\sum_{k=1}^{m}\left[(1-\hat{M}_{ij})\left(x_{j}-y_{i}(x,\hat{M})\right)^{+}+\hat{M}_{ij}\left|y_{i}(x,\hat{M})-y_{i}(x,\hat{M}_{-ij},M_{ij}=0)\right|\right]\frac{\partial}{\partial y_{i}}C(y(x,\hat{M}))
\]

\end_inset


\end_layout

\begin_layout Standard
Now consider the case where 
\begin_inset Formula $\hat{M_{ij}}=1$
\end_inset

.
 In this case, 
\begin_inset Formula $y_{i}=\max(x_{j},S_{ij})$
\end_inset

 so if 
\begin_inset Formula $x_{j}<y_{i}$
\end_inset

 we know 
\begin_inset Formula $S_{ij}>x_{j}$
\end_inset

.
 So this puts us in category 1 right off the bat.
 It's also not possible that 
\begin_inset Formula $x_{j}>y_{i}$
\end_inset

.
 That leaves 
\begin_inset Formula $x_{j}=y_{i}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\frac{1}{m}\sum_{k=1}^{m}\left[(1-\hat{M}_{ij})\left(x_{j}-y_{i}(x,\hat{M})\right)^{+}+\hat{M}_{ij}\mathbb{I}(x_{j}=y_{i}(x,\hat{M}))\left(x_{j}-y_{i}(x,\hat{M}_{-ij},M_{ij}=0)\right)\right]\frac{\partial}{\partial y_{i}}C(y(x,\hat{M}))
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\frac{1}{m}\sum_{k=1}^{m}\left[(1-\hat{M}_{ij})\left(x_{j}-y_{i}(x,\hat{M})\right)^{+}+\hat{M}_{ij}\mathbb{I}(x_{j}=y_{i}(x,\hat{M}))\left(x_{j}-S_{ij}\right)\right]\frac{\partial}{\partial y_{i}}C(y(x,\hat{M}))
\]

\end_inset


\end_layout

\begin_layout Standard
It looks like I can't really get around implementing 
\begin_inset Formula $S_{ij}$
\end_inset

.
 So maybe I should just bite the bullet and write some C code.
 Suppose I write an op that returns both 
\begin_inset Formula $y(x,M)$
\end_inset

 and 
\begin_inset Formula $f(x,M)_{ij}=(1-M_{ij})\left(x_{j}-y_{i}(x,M)\right)^{+}+M_{ij}\mathbb{I}(x_{j}=y_{i}(x,M))\left(x_{j}-S_{ij}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
Then the gradient is written in terms of:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\frac{1}{m}\sum_{k=1}^{m}f(x,\hat{M})_{ij}\frac{\partial}{\partial y_{i}}C(y(x,\hat{M}))
\]

\end_inset


\end_layout

\begin_layout Standard
So the gradient as a whole is just computing 
\begin_inset Formula $f$
\end_inset

 and elemwise multiplying it with the gradient of 
\begin_inset Formula $C$
\end_inset

 broadcasted in one direction.
\end_layout

\begin_layout Section
An alternate view
\end_layout

\begin_layout Standard
The unbiased estimate of the gradient is
\end_layout

\begin_layout Standard
\begin_inset Formula $=\frac{1}{m}\sum_{k=1}^{m}\left[C(y(x,\hat{M}_{-ij}^{(k)},M_{ij}=1))-C(y(x,\hat{M}_{-ij}^{(k)},M_{ij}=0))\right]$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $=\frac{1}{m}\sum_{k=1}^{m}\left[C(y(x,\hat{M}_{-ij}^{(k)},M_{ij}=1))\right]-\frac{1}{m}\sum_{k=1}^{m}\left[C(y(x,\hat{M}_{-ij}^{(k)},M_{ij}=0))\right]$
\end_inset


\end_layout

\begin_layout Standard
So another unbiased estimator, which only works when all values have nonzero
 probability of being sampled, is
\end_layout

\begin_layout Standard
\begin_inset Formula $=\frac{1}{\sum_{k}\hat{M}_{ij}^{(k)}}\sum_{k=1}^{m}C(y(x,\hat{M}^{(k)}))\hat{M}_{ij}^{(k)}-\frac{1}{\sum_{k}1-\hat{M}_{ij}^{(k)}}\sum_{k=1}^{m}C(y(x,\hat{M}^{(k)}))(1-\hat{M}_{ij}^{(k)})$
\end_inset


\end_layout

\begin_layout Standard
This is unbiased because the different elements of 
\begin_inset Formula $M$
\end_inset

 are independent so rejecting samples based on the value of 
\begin_inset Formula $M_{ij}$
\end_inset

 won't skew the distribution of 
\begin_inset Formula $M_{-ij}$
\end_inset

.
\end_layout

\begin_layout Standard
This shifts the burden from evaluating 
\begin_inset Formula $C$
\end_inset

 at multiple values of 
\begin_inset Formula $y$
\end_inset

 per sample to sampling until both values of every element of 
\begin_inset Formula $M$
\end_inset

 have been observed at least once.
 (If you use 0/0=1 then the gradient disappears on deterministic parameters
 so they can never change after becoming deterministic)
\end_layout

\begin_layout Standard
It might be possible to fix this by introducing some 
\begin_inset Quotes eld
\end_inset

leak probability
\begin_inset Quotes erd
\end_inset

 into the sampling and adding some correction factor to the update.
 But this is still probably not as good as Yoshua's first order approximation
 trick, since the sampling method only has the chance to 
\begin_inset Quotes eld
\end_inset

see
\begin_inset Quotes erd
\end_inset

 the possible benefit of change.
\end_layout

\begin_layout Section
Thoughts on the first order approximation
\end_layout

\begin_layout Standard
It will be most accurate when the values of 
\begin_inset Formula $x$
\end_inset

 are finely spaced, so that the jumps in 
\begin_inset Formula $y$
\end_inset

 will be smaller.
 It will be most accurate when the cost function is not too curved.
 This is one reason I want to start out using it with hinge loss.
\end_layout

\begin_layout Standard
It can underestimate the benefit of a jump if it's in a locally flat region.
 But ordinary gradient descent has this problem too.
\end_layout

\begin_layout Standard
It can seriously overestimate the value of a change if a lot of the value
 of that change comes from increasing the chance of making a big jump.
 The local gains associated with that jump direction probably don't continue
 for very far.
 This is one disadvantage compared to correct gradient descent.
\end_layout

\begin_layout Section
Integrating this into theano
\end_layout

\begin_layout Standard
Assuming that 
\begin_inset Formula $I$
\end_inset

 is a parameter and not a function of something else, there is no need to
 compute gradients through it.
 So we can just use this formula to do the
\end_layout

\begin_layout Standard
gradient on 
\begin_inset Formula $M$
\end_inset

, and the regular theano machinery to do the gradient on the other parameters.
\end_layout

\end_body
\end_document
