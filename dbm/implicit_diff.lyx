#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section
Implicit differentiation
\end_layout

\begin_layout Standard
DBM mean field inference is defined by the fixed point equations
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\hat{h}_{i}=\sigma\left(b_{i}+v^{T}W_{:i}+V_{i:}\hat{g}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
We can solve for the derivative
\begin_inset Formula 
\[
\frac{\partial\hat{h}_{i}}{\partial W_{ji}}
\]

\end_inset


\end_layout

\begin_layout Standard
by deriving both sides of the equation with respect to 
\begin_inset Formula $W_{ji}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\partial\hat{h}_{i}}{\partial W_{ji}}=\hat{h}_{i}(1-\hat{h}_{i})[v_{j}+\sum_{k}V_{ik}\frac{\partial\hat{g}_{k}}{\partial W_{ji}}]
\]

\end_inset


\end_layout

\begin_layout Standard
This is pretty nasty.
 We have two problems here:
\end_layout

\begin_layout Standard
-We need to differentiate every variational parameter with respect to every
 model parameter (except the visible biases).
 So our memory consumption is state size 
\begin_inset Formula $\times$
\end_inset

 model size.
 We can't fit that in RAM.
\end_layout

\begin_layout Standard
-We need to iterate fixed point updates to get these derivatives
\end_layout

\begin_layout Standard
Maybe figuring out how to get around the second will show how to get around
 the first.
 How did Drew Bagnell do it?
\end_layout

\begin_layout Standard
Maybe can we end up getting out of the problem with algebra?
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\partial\hat{g}_{k}}{\partial W_{ji}}=\hat{g}_{k}(1-\hat{g}_{k})\sum_{l}V_{lk}\frac{\partial\hat{h}_{l}}{\partial W_{ji}}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\partial\hat{h}_{i}}{\partial W_{ji}}=\hat{h}_{i}(1-\hat{h}_{i})[v_{j}+\sum_{k}V_{ik}\hat{g}_{k}(1-\hat{g}_{k})\sum_{l}V_{lk}\frac{\partial\hat{h}_{l}}{\partial W_{ji}}]
\]

\end_inset


\end_layout

\begin_layout Standard
rewriting with some simpler letters we get:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
X_{iji}=a_{i}(v_{j}+\sum_{k}V_{ik}c_{k}\sum_{l}V_{lk}X_{lji})
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
X_{iji}=a_{i}(v_{j}+\sum_{k}V_{ik}c_{k}\sum_{l\neq i}V_{lk}X_{lji})+X_{iji}a_{i}\sum_{k}V_{ik}^{2}c_{k}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
X_{iji}=\frac{a_{i}(v_{j}+\sum_{k}V_{ik}c_{k}\sum_{l\neq i}V_{lk}X_{lji})}{1-a_{i}\sum_{k}V_{ik}^{2}c_{k}}
\]

\end_inset


\end_layout

\begin_layout Section
Sparsity penalty
\end_layout

\begin_layout Standard
So, what is the right sparsity penalty to apply?
\end_layout

\begin_layout Standard
We tend to think in terms of binary cross-entropy applied to some running
 average of the variational parameters.
 But really what we're regularizing is a bunch of mean field parameters
 in [0,1].
 Ideally we'd like a prior that encourages these to be usually close to
 0 and occasionally close to 1.
 This could probably be done with a mixture of two beta distributions.
\end_layout

\end_body
\end_document
