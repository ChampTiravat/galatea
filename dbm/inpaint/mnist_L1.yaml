#MNIST L is doing fairly well, but if you take a gibbs step in it the result is junk
#This makes me think the reconstruction depends too much on using continuous-valued
#hidden units
#One way to fix this is to encourage the hidden units to be more binary
#I tried doing that in mnist_N, mnist_O by just adding an h(1-h) penalty
#I've noticed that also the weights seem to connect to too many units, so
#maybe a contractive penalty would be better
!obj:pylearn2.scripts.train.Train {
    dataset: &data !obj:pylearn2.datasets.mnist.MNIST {
                        which_set : 'train',
                        binarize: 1,
                        shuffle: 1 },
    model: !obj:pylearn2.models.dbm.DBM {
              rbms: [
                     !obj:pylearn2.models.rbm.RBM {
                     init_bias_vis_marginals: *data,
                     nvis : &nvis 784,
                     nhid : &nh 500,
                     irange : 0.05,
                     },
                     !obj:pylearn2.models.rbm.RBM {
                     nvis : *nh,
                     nhid : 1000,
                     irange : 0.05,
                     }
                     ],
             use_cd : 1, #this is just to tell it not to do negative chains
    },
    algorithm: !obj:galatea.dbm.inpaint.inpaint_alg.InpaintAlgorithm {
               batch_size : 4000,
               batches_per_iter : 10,
               monitoring_batches : 1,
               monitoring_dataset : *data,
               cost : !obj:galatea.dbm.inpaint.dbm_inpaint.DBM_Inpaint_Binary {
                        n_iter : 10,
                        weight_decay : 0.0,
                        reweight : 0,
                        reweight_correctly: 1,
                        recons_penalty: 1.,
                        h_contractive_penalty: .1,
                        g_contractive_penalty: .1,
                        h_penalty : 0.0,
                        h_target: .1,
                        g_penalty: 0.0,
                        g_target: .1
               },
               mask_gen : !obj:galatea.dbm.inpaint.dbm_inpaint.MaskGen {
                        drop_prob: 0.5,
                        balance: 1
               }
        },
    save_path: "${PYLEARN2_TRAIN_FILE_NAME}.pkl",
    save_freq : 1
}

