# like cifar100_A4
# but on cifar10
!obj:pylearn2.train.Train {
    dataset: &data !obj:galatea.datasets.zca_dataset.ZCA_Dataset {
        preprocessed_dataset: !pkl: "/data/lisa/data/cifar10/pylearn2_gcn_whitened/train.pkl",
        preprocessor: !pkl: "/data/lisa/data/cifar10/pylearn2_gcn_whitened/preprocessor.pkl"
    },
    model: !obj:galatea.dbm.inpaint.super_dbm.SuperDBM {
              batch_size : 150,
              niter: 6, #note: since we have to backprop through the whole thing, this does
                         #increase the memory usage
              visible_layer: !obj:galatea.dbm.inpaint.super_dbm.GaussianConvolutionalVisLayer {
                rows: 32,
                cols: 32,
                channels: 3,
                init_beta: 3.7,
                init_mu: 0.
              },
              hidden_layers: [
                                !obj:galatea.dbm.inpaint.super_dbm.ConvMaxPool {
                                        scale_by_sharing: 0,
                                        border_mode : 'full',
                                        output_channels: 64,
                                        kernel_rows: 5,
                                        kernel_cols: 5,
                                        pool_rows: 3,
                                        pool_cols: 3,
                                        irange: 0.05,
                                        layer_name: 'h0_conv',
                                        init_bias: -5.
                               },
                                !obj:galatea.dbm.inpaint.super_dbm.ConvMaxPool {
                                        scale_by_sharing: 0,
                                        border_mode : 'valid',
                                        output_channels: 64,
                                        kernel_rows: 4,
                                        kernel_cols: 4,
                                        pool_rows: 3,
                                        pool_cols: 3,
                                        irange: 0.05,
                                        layer_name: 'h1_conv',
                                        init_bias: -3.
                               }
                               ],
    },
    algorithm: !obj:galatea.dbm.inpaint.inpaint_alg.InpaintAlgorithm {
               conjugate: 1,
               line_search_mode: 'exhaustive',
               reset_conjugate: 0,
               reset_alpha: 0,
               batches_per_iter : 10,
               monitoring_batches : 1,
               monitoring_dataset : *data,
               init_alpha : [ 1e-3, 1e-2, 1e-1, 2e-1, 1.],
               max_iter: 5,
               cost : !obj:pylearn2.costs.cost.SumOfCosts {
                costs: [
                        !obj:galatea.dbm.inpaint.super_inpaint.SuperInpaint {
                                both_directions : 0,
                                noise : 0,
                                l1_act_targets: [ [.06, 1.0],
                                                  [.06, 1.0]],
                                l1_act_eps: [ [.02, .0],
                                              [.06, .0]],
                                l1_act_coeffs:[ [1., 0.],
                                                [.0001, 0.]],
                        },
                        !obj:galatea.dbm.inpaint.super_dbm.DBM_WeightDecay {
                                coeffs: [ .0000005, 0. ]
                        }
                       ]
               },
               mask_gen : !obj:galatea.dbm.inpaint.super_inpaint.MaskGen {
                        drop_prob: 0.1,
                        balance: 0,
                        sync_channels: 0
               }
        },
    save_path: "${PYLEARN2_TRAIN_FILE_FULL_STEM}.pkl",
    save_freq : 1,
    allow_overwrite: 0
}

