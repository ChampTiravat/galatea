# Like mnist_sup_inpaint_P (well, the redos of it) 
# but with combine_batches feature
!obj:pylearn2.train.Train {
    dataset: &train !obj:pylearn2.datasets.mnist.MNIST {
        which_set: "train",
        shuffle: 0,
        binarize: 1,
        one_hot: 1,
        start: 0,
        stop: 50000
    },
    model: !obj:galatea.dbm.inpaint.super_dbm.SuperDBM {
              batch_size : 500,
              niter: 6, #note: since we have to backprop through the whole thing, this does
                         #increase the memory usage
              visible_layer: !obj:galatea.dbm.inpaint.super_dbm.BinaryVisLayer {
                nvis: 784,
                bias_from_marginals: *train,
              },
              hidden_layers: [
                !obj:galatea.dbm.inpaint.super_dbm.DenseMaxPool {
                        detector_layer_dim: %(layer_1_dim)d,
                        pool_size: 1,
                        sparse_init: %(layer_1_sparse_init)d,
                        layer_name: 'h0',
                        init_bias: 0.
               },
                !obj:galatea.dbm.inpaint.super_dbm.DenseMaxPool {
                        detector_layer_dim: %(layer_2_dim)d,
                        pool_size: 1,
                        sparse_init: %(layer_2_sparse_init)d,
                        layer_name: 'h1',
                        init_bias: 0.
               },
               !obj:galatea.dbm.inpaint.super_dbm.Softmax {
                        sparse_init: %(class_sparse_init)d,
                        copies: %(copies)d,
                        layer_name: 'c',
                        n_classes: 10
               }
              ]
    },
    algorithm: !obj:galatea.dbm.inpaint.inpaint_alg.InpaintAlgorithm {
               duplicate: %(duplicate)d,
               combine_batches : 2,
               monitoring_dataset : {
                        'train': *train,
                        'valid': !obj:pylearn2.datasets.mnist.MNIST {
                                which_set: "train",
                                shuffle: 0,
                                binarize: 1,
                                one_hot: 1,
                                start: 50000,
                                stop: 60000
                            },
               },
               line_search_mode: 'exhaustive',
               init_alpha : [0.0256, .128, .256, 1.28, 2.56],
               reset_alpha: 0,
               conjugate: 1,
               reset_conjugate: 0,
               max_iter: %(max_iter)d,
               cost: !obj:pylearn2.costs.cost.SumOfCosts {
                       costs :[
                               !obj:galatea.dbm.inpaint.super_inpaint.SuperInpaint {
                                        both_directions : 0,
                                        noise : 0,
                                        supervised: 1,
                                        range_rewards: [ 0., %(layer_2_range_rewards)e, 0. ],
                                        l1_act_targets: [  %(layer_1_target)e, %(layer_2_target)e, 0. ],
                                        l1_act_eps:     [  %(layer_1_eps)e,  %(layer_2_eps)e, 0. ],
                                        l1_act_coeffs:  [  %(layer_1_coeff)e,
                                           &l2_sparse_coeff !obj:pylearn2.utils.sharedX {
                                                              value: 0.,
                                                                                name: 'l2_sparse_coeff' }, 0.  ]
                               },
                               !obj:galatea.dbm.inpaint.super_dbm.DBM_WeightDecay {
                                        coeffs: [ &weight_cost !obj:pylearn2.utils.sharedX {
                                                value: %(wd_before)e,
                                                name: 'weight_cost'
                                                },
                                        *weight_cost, *weight_cost ]
                               }
                       ]
               },
               mask_gen : !obj:galatea.dbm.inpaint.super_inpaint.MaskGen {
                        drop_prob: %(drop_prob)e,
                        balance: 0,
                        sync_channels: 0
               },
               termination_criterion: &batch_grower !obj:galatea.dbm.inpaint.inpaint_alg.BatchGrower
               {
                        channel: "valid_objective",
                        available_batches: 100,
                        reset_best: [ &activate %(activate)d ]
               }
        },
    extensions: [
                *batch_grower,
                !obj:pylearn2.train_extensions.SharedSetter {
                        epoch_updates: [
                                [ *activate, *l2_sparse_coeff, %(layer_2_coeff)e ],
                                [ *activate, *weight_cost, %(wd_after)e ]
                        ]
                },
                !obj:pylearn2.train_extensions.best_params.MonitorBasedSaveBest {
                        channel_name: "valid_err",
                        save_path: "${PYLEARN2_TRAIN_FILE_FULL_STEM}_best.pkl"
                }
        ],
    save_path: "${PYLEARN2_TRAIN_FILE_FULL_STEM}.pkl",
    save_freq : 1
}

