# benchmark the supervised training
!obj:pylearn2.scripts.train.Train {
    dataset: &data !obj:galatea.datasets.zca_dataset.ZCA_Dataset {
        preprocessed_dataset: !obj:pylearn2.datasets.dense_design_matrix.from_dataset {
                dataset: !pkl: "/data/lisa/data/cifar10/pylearn2_gcn_whitened/train.pkl",
                num_examples: 1000
                },
        preprocessor: !pkl: "/data/lisa/data/cifar10/pylearn2_gcn_whitened/preprocessor.pkl"
    },
        model: !obj:galatea.dbm.inpaint.super_dbm.ditch_mu {
          model: !obj:galatea.dbm.inpaint.super_dbm.add_layers {
                 super_dbm: !obj:galatea.dbm.inpaint.super_dbm.SuperDBM {
              batch_size : 25,
              niter: 6, #note: since we have to backprop through the whole thing, this does
                         #increase the memory usage
              visible_layer: !obj:galatea.dbm.inpaint.super_dbm.GaussianConvolutionalVisLayer {
                rows: 32,
                cols: 32,
                channels: 3,
                init_beta: 3.7,
                init_mu: 0.
              },
              hidden_layers: [
                !obj:galatea.dbm.inpaint.super_dbm.ConvMaxPool {
                        border_mode : 'full',
                        output_axes : ['b', 'c', 0, 1],
                        output_channels: 64,
                        kernel_rows: 8,
                        kernel_cols: 8,
                        pool_rows: 3,
                        pool_cols: 3,
                        irange: 0.05,
                        layer_name: 'h0',
                        init_bias: -5.
               },
                !obj:galatea.dbm.inpaint.super_dbm.ConvMaxPool {
                        border_mode : 'full',
                        output_channels: 64,
                        output_axes: ['b', 'c', 0, 1],
                        kernel_rows: 6,
                        kernel_cols: 6,
                        pool_rows: 3,
                        pool_cols: 3,
                        irange: 0.3,
                        layer_name: 'h1',
                        init_bias: -5.
               }
              ]
    },
                 new_layers: [
                !obj:galatea.dbm.inpaint.super_dbm.Softmax {
                        irange: 0.05,
                        n_classes: 10,
                        layer_name: 'class_layer',
               }
              ]
              }},
    algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {
               termination_criterion: !obj:pylearn2.training_algorithms.sgd.EpochCounter {
                max_epochs: 1
               },
               learning_rate: 1e-3,
               init_momentum: .5,
               monitoring_batches : 1,
               monitoring_dataset : *data,
               cost : !obj:galatea.dbm.inpaint.super_dbm.SuperDBM_ConditionalNLL {
               },
        },
    callbacks: [ !obj:pylearn2.training_algorithms.sgd.MomentumAdjustor {
                        start: 0,
                        saturate: 10,
                        final_momentum: .9
                  }
                ],
    save_freq : 0
}

