#early attempt at using an E-step based on mean field in the original model
#E2A is not actually doing theoretically impossible things
#I'm confident that
#1. Our M-step is guaranteed to never decrease the EM functional
#2. Our E-step in practice never decreases the EM functional.... STARTING
#      FROM THE PRIOR. But what we actually need is for it to never
#      decrease the EM functional starting from the value of q on the
#      previous iteration.
!obj:pylearn2.scripts.train.Train {
    "save_path": "attempt_E2A1A.pkl",
    "dataset": !pkl: "/data/lisatmp/goodfeli/cifar10_preprocessed_train_1K.pkl",
    "model": !obj:galatea.s3c.s3c.S3C {
               "nvis" : 192,
               "nhid" : 300,
               "init_bias_hid" : -1.5,
               "irange"  : .02,
               "init_B"  : 3.,
               "min_B"   : 1e-8,
               "max_B"   : 10000,
               "init_alpha" : 1.,
               "min_alpha" : 1.,
               "max_alpha" : 1000.,
               "init_mu" : 1.,
               "monitor_functional" : 1,
               "recycle_q" : 1000,
               "e_step" : !obj:galatea.s3c.s3c.VHS_E_Step {
                        "h_new_coeff_schedule" : [ .01 ],
                        "monitor_em_functional" : 1,
                        "monitor_kl" : 0
               },
               "new_stat_coeff" : 1.,
               #"learn_after" : 1000,
               "m_step"     : !obj:galatea.s3c.s3c.VHS_Solve_M_Step {
                        "new_coeff" : 1.
                        #"learning_rate" : 1e-3
               },
               "mu_eps" : 0.,
               "b_eps"  : 1e-3
        },
    "algorithm": !obj:pylearn2.training_algorithms.default.DefaultTrainingAlgorithm {
               "batch_size" : 1000,
               "batches_per_iter" : 1,
               "monitoring_batches" : 1,
               "monitoring_dataset" : !pkl: "/data/lisatmp/goodfeli/cifar10_preprocessed_train_1K.pkl",
        },
}

