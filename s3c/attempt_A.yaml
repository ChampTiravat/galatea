#early attempt at using an E-step based on mean field in the original model
#attempt_009 shrinks the weights. in this attempt, we initialize B very small so that the gradient on the weights is stronger far away from the modes they define (since the modes will initially be far from the data). we also set W_eps to 0
#also new: tied B
!obj:pylearn2.scripts.train.Train {
    "save_path": "attempt_A.pkl",
    "dataset": !pkl: "/data/lisatmp/goodfeli/cifar10_preprocessed_train_2M.pkl",
    "model": !obj:galatea.s3c.s3c.S3C {
               "nvis" : 192,
               "nhid" : 300,
               "init_bias_hid" : -1.5,
               "irange"  : .5,
               "tied_B"  : 1,
               "init_B"  : 1e-8,
               "min_B"   : 1e-8,
               "max_B"   : 10000,
               "init_alpha" : 1.,
               "min_alpha" : 1.,
               "max_alpha" : 1000.,
               "init_mu" : 1.,
               "e_step" : !obj:galatea.s3c.s3c.VHS_E_Step {
                        "h_new_coeff_schedule" : [ .1, .2, .3, .4, .5, .6, .7, .8, .9, 1. ]
               },
               "new_stat_coeff" : .005,
               "learn_after" : 1000,
               "m_step"     : !obj:galatea.s3c.s3c.VHS_Solve_M_Step {
                        "new_coeff" : .001
                        #"learning_rate" : 1e-3
               },
               "mu_eps" : 0.,
               "W_eps" : 0.
        },
    "algorithm": !obj:pylearn2.training_algorithms.default.DefaultTrainingAlgorithm {
               "batch_size" : 500,
               "batches_per_iter" : 10,
               "monitoring_batches" : 1,
               "monitoring_dataset" : !pkl: "/data/lisatmp/goodfeli/cifar10_preprocessed_train_2M.pkl",
        }
}

