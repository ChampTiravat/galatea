import theano.tensor as T
from pylearn2.costs.cost import Cost
from theano.printing import Print
from pylearn2.expr.nnet import softmax_ratio
from pylearn2.utils import block_gradient

class BoostTry1(Cost):
    """
    This isn't thought through all that carefully, probably not correct at all
    """

    supervised = True

    def __call__(self, model, X, Y, **kwargs):

        Y = Y * 2 - 1

        # Get the approximate ensemble predictions
        Y_hat = model.fprop(X, apply_dropout=False)
        # Pull out the argument to the sigmoid
        assert hasattr(Y_hat, 'owner')
        owner = Y_hat.owner
        assert owner is not None
        op = owner.op

        if not hasattr(op, 'scalar_op'):
            raise ValueError("Expected Y_hat to be generated by an Elemwise op, got "+str(op)+" of type "+str(type(op)))
        assert isinstance(op.scalar_op, T.nnet.sigm.ScalarSigmoid)
        F ,= owner.inputs

        weights = - Y * T.nnet.softmax(-(Y * F).T).T

        weights = block_gradient(weights)


        # Get the individual model predictions
        Y_hat = model.fprop(X, apply_dropout=True)
        # Pull out the argument to the sigmoid
        assert hasattr(Y_hat, 'owner')
        owner = Y_hat.owner
        assert owner is not None
        op = owner.op

        if not hasattr(op, 'scalar_op'):
            raise ValueError("Expected Y_hat to be generated by an Elemwise op, got "+str(op)+" of type "+str(type(op)))
        assert isinstance(op.scalar_op, T.nnet.sigm.ScalarSigmoid)
        f ,= owner.inputs

        cost = (weights * T.exp(-Y * f)).mean()

        assert cost.ndim == 0

        return cost

class BoostTry2(Cost):
    """
    This isn't thought through all that carefully, probably not correct at all
    """

    supervised = True

    def __call__(self, model, X, Y, **kwargs):

        Y_hat = model.fprop(X, apply_dropout=False)
        prob = Y_hat * Y + (1-Y_hat) * (1-Y)

        weight = 1./(.1 + prob)

        weight = block_gradient(weight)

        Y_hat = model.fprop(X, apply_dropout=True)
        # Pull out the argument to the sigmoid
        assert hasattr(Y_hat, 'owner')
        owner = Y_hat.owner
        assert owner is not None
        op = owner.op

        if not hasattr(op, 'scalar_op'):
            raise ValueError("Expected Y_hat to be generated by an Elemwise op, got "+str(op)+" of type "+str(type(op)))
        assert isinstance(op.scalar_op, T.nnet.sigm.ScalarSigmoid)
        Z ,= owner.inputs

        term_1 = Y * T.nnet.softplus(-Z)
        term_2 = (1 - Y) * T.nnet.softplus(Z)

        total = term_1 + term_2

        total = weight * total

        ave = total.mean()

        return ave

#Try3 had a bug

class BoostTry4(Cost):

    supervised = True

    def __init__(self, k = 1, alpha = 1, beta =1):
        self.k = k
        self.alpha = alpha
        self.beta = beta

    def get_weight(self, model, X, Y):

        ensemble_Y = model.fprop(X, apply_dropout=False)
        prob_of = (ensemble_Y * Y).sum(axis=1)

        weight = 1./ (self.k + self.alpha * (prob_of - self.beta * 1./T.cast(Y.shape[1], 'float32')))
        weight = weight / weight.sum()
        weight = block_gradient(weight)
        return weight

    def get_monitoring_channels(self, model, X, Y, ** kwargs):

        weight = self.get_weight(model, X, Y)

        return { 'weight_min': weight.min(),
                'weight_max': weight.max(),
                'weight_mean' : weight.mean() }


    def __call__(self, model, X, Y, **kwargs):

        weight = self.get_weight(model, X, Y)

        Y_hat = model.fprop(X, apply_dropout=True)

        assert hasattr(Y_hat, 'owner')
        owner = Y_hat.owner
        assert owner is not None
        op = owner.op
        if isinstance(op, Print):
            assert len(owner.inputs) == 1
            Y_hat, = owner.inputs
            owner = Y_hat.owner
            op = owner.op
        assert isinstance(op, T.nnet.Softmax)
        z ,= owner.inputs
        assert z.ndim == 2

        z = z - z.max(axis=1).dimshuffle(0, 'x')
        log_prob = z - T.log(T.exp(z).sum(axis=1).dimshuffle(0, 'x'))
        # we use sum and not mean because this is really one variable per row
        log_prob_of = (Y * log_prob).sum(axis=1)
        assert log_prob_of.ndim == 1

        weighted_log_prob_of = T.dot(weight, log_prob_of)


        return - weighted_log_prob_of


class EnsembleLikelihoodTrainOne(Cost):

    supervised = True

    def __call__(self, model, X, Y, **kwargs):

        Y_hat_e = model.fprop(X)
        Y_hat = model.fprop(X, apply_dropout=True)

        softmax_r = softmax_ratio(Y_hat_e, Y_hat)

        softmax_r = block_gradient(softmax_r)

        neg_terms = softmax_r * Y_hat

        neg = - neg_terms.sum(axis=1).mean(axis=0)

        assert hasattr(Y_hat, 'owner')
        owner = Y_hat.owner
        assert owner is not None
        op = owner.op
        if isinstance(op, Print):
            assert len(owner.inputs) == 1
            Y_hat, = owner.inputs
            owner = Y_hat.owner
            op = owner.op
        assert isinstance(op, T.nnet.Softmax)
        z ,= owner.inputs
        assert z.ndim == 2

        z = z - z.max(axis=1).dimshuffle(0, 'x')
        log_prob = z - T.log(T.exp(z).sum(axis=1).dimshuffle(0, 'x'))
        # we use sum and not mean because this is really one variable per row
        log_prob_of = (Y * log_prob).sum(axis=1)
        assert log_prob_of.ndim == 1
        log_prob_of = log_prob_of.mean()

        return -(log_prob_of + neg)

class PoE_SameMask(Cost):

    supervised = True

    def __init__(self, alpha = 1):
        self.alpha = alpha

    def __call__(self, model, X, Y, **kwargs):

        Y_hat_e = model.fprop(X)
        Y_hat = model.fprop(X, apply_dropout=True)

        assert hasattr(Y_hat, 'owner')
        owner = Y_hat.owner
        assert owner is not None
        op = owner.op
        if isinstance(op, Print):
            assert len(owner.inputs) == 1
            Y_hat, = owner.inputs
            owner = Y_hat.owner
            op = owner.op
        assert isinstance(op, T.nnet.Softmax)
        z ,= owner.inputs
        assert z.ndim == 2

        z_weight = Y_hat - Y_hat_e
        z_weight = block_gradient(z_weight)
        neg = z_weight * z
        neg = neg.sum(axis=1).mean()

        z = z - z.max(axis=1).dimshuffle(0, 'x')
        log_prob = z - T.log(T.exp(z).sum(axis=1).dimshuffle(0, 'x'))
        # we use sum and not mean because this is really one variable per row
        log_prob_of = (Y * log_prob).sum(axis=1)
        assert log_prob_of.ndim == 1
        log_prob_of = log_prob_of.mean()

        return -(log_prob_of + self.alpha * neg)

