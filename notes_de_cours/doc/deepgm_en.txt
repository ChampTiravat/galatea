:ref:`Version française <deepgm_fr>`

.. _deepgm_en:

Probabilistic models for deep architectures
===========================================

Of particular interest are Boltzmann machine models, certain
variants of which are used in deep architectures such as
*Deep Belief Networks* and *Deep Boltzmann Machines*.
See section 5 of `Learning Deep Architectures for AI <http://www.iro.umontreal.ca/%7Elisa/publications2/index.php/publications/show/239>`_.

The Boltzmann distribution is generally defined on binary variables
:math:`x_i \in \{0,1\}`, with

.. math::

 P(x) = \frac{e^{x' W x + b'x}}{\sum_{\tilde x} \tilde{x}' W \tilde{x} + b'\tilde{x}}

where the denominator is simply a normalizing constant such that
:math:`\sum_x P(x)=1`, and the :math:`W_{ij}` indicates the nature
of the interaction (e.g. a positive value indicates that :math:`x_i`
et :math:`x_j` prefer to take the same value)  between pairs of
variables, and :math:`b_i` indicates the inclination of a given
:math:`x_i` to take a value of 1.


Readings on probabilistic graphical models
------------------------------------------

See

`Graphical models: probabilistic inference <http://www.cs.berkeley.edu/~jordan/papers/jordan-weiss.ps>`_.
M. I. Jordan and Y. Weiss. In M. Arbib (Ed.), The Handbook of Brain Theory and Neural Networks, 2nd edition.
Cambridge, MA: MIT Press, 2002.


Certain distributions can be written :math:`P(x)` for
a vector of variables :math:`x=(x_1,x_2,\ldots)` in the form

.. math::
 P(x) = \frac{1}{Z} \prod_c \psi_c(x_c)

where :math:`Z` is the normalizing constant (called the **partition function**),
and the product is over *cliques* (subsets :math:`x_c` of elements of the vector :math:`x`),
and the :math:`\psi_c(.)` are functions (one per clique) that indicate how
the variables in each clique interact.

A particular case where :math:`Z` may be simplified a bit (factorized
over cliques) is the case of *directed models* where variables
are structured as a directed acyclic graph, with a topological ordering
that associates a group of *parent variables* :math:`parents(x_i)` with
each variable :math:`x_i`:

.. math::
 P(x) = \prod_i P_i(x_i | parents(x_i))

where it can be seen that there is one clique for variable and its parents,
i.e., :math:`P_i(x_i | parents(x_i)) = \psi_i(x_i, parents(x_i))/Z_i`.

In the general case (represented with an undirected graph), the
potential functions :math:`\psi_c` are directly parameterized,
often in the space of logarithms of :math:`\psi_c`,
leading to a formulation known as a **Markov random field**:

.. math::
 P(x) = \frac{1}{Z} e^{-\sum_c E_c(x_c)}

where:math:`E(x)=\sum_c E_c(x_c)` is called the **energy function**.
The energy function  of a Boltzmann machine is a second degree polynomial
in :math:`x`. The most common parameterization of Markov random fields
has the following form, which is **log-linear**:

.. math::
 P(x) = \frac{1}{Z} e^{-\sum_c \theta_c f_c(x_c)}

where the only free parameters are the :math:`\theta_c`,
and where the complete log likelihood (when :math:`x`
est completely observed in each training example) est *log-linear*
in the parameters :math:`\theta`. One can easily
show that this function is  **convex** in :math:`\theta`.


Inference
---------

One of the most important obstacles in the practical application of
the majority of probabilistic models is the difficulty of **inference**:
given certain variables (a subset of :math:`x`),
predict the marginal distribution (separately for each) or joint
distribution of certain other variables.
Let :math:`x=(v,h)` with :math:`h` (*hidden*) being the variables
we would like to predict, and :math:`v` (*visible*)
being the observed subset.
One would like to calculate, or at least sample from,

.. math::
   P(h | v).

Inference is obviously useful if certain variables are missing, or
if, while using the model, we wish to predict a certain variable
(for example the class of an image) given some other variables
(for example, the image itself). Note that if the model
has hidden variables (variables that are *never* observed in the data)
we do not try to predict the values directly
qu'on ne cherche pas à prédire directement, but we will still implicitly
*marginalize* over these variables (sum over all configurations
of these variables).

Inference is also an essential component of learning, in order to
calculate gradients (as seen below in the case of Boltzmann
machines) or in the use of the Expectation-Maximization (EM) algorithm
which requires a marginalization over all hidden variables.

in general, exact inference has a computational cost exponential
in the size of the cliques of a graph (in fact, the unobserved
part of the graph) because we must consider all possible combinations
of values of the variables in each clique. See section 3.4
of `Graphical models: probabilistic inference <http://www.cs.berkeley.edu/~jordan/papers/jordan-weiss.ps>`_
for a survey of exact inference methods.

A simplifed form ofinference consists of calculatign not the entire
distribution, but only the mode (the most likely configuration of
values) of the distribution:

.. math::
   h^* = {\rm argmax}_{h} P(h | v)

This is known as **MAP = Maximum A Posteriori** inference.

Approximate inference
---------------------

The two principal families of methods for approximate inference in
probabilistic models are Markov chain Monte Carlo (MCMC) methods and
variational inference.

The principle behind variational inference is the following. We will define a
simpler model than the target model (the one that interests us), in which
inference is easy, with a similar set of variables (though generally with more
simple dependencies between variables than those contained in the target
model). We then optimize the parameters of the simpler model so as to
approximate the target model as closely as possible. Finally, we do inference
using the simpler model.  See section 4.2 de `Graphical models: probabilistic
inference <http://www.cs.berkeley.edu/~jordan/papers/jordan-weiss.ps>`_ for
more details and a survey.

Inference with MCMC
-------------------

In general :math:`P(h | v)` can be exponentially expensive to represent (in
terms of the number of hidden variables, because we must consider all possible
configurations of :math:`h`).  The principle behind Monte Carlo inference is
that we can approximate the distribution :math:`P(h | v)` using samples from
this distribution.  Indeed, in practice we only need an expectation (for
example, the expectation of the gradient) under this conditional distribution.
We can thus approximate the desired expectation with an average of these
samples.

See the page `site du zéro sur Monte-Carlo
<http://www.siteduzero.com/tutoriel-3-133680-toute-la-puissance-de-monte-carlo.html>`_
(in French) for a gentle introduction.

Unfortunately, for most probabilistic models, even sampling from :math:`P(h |
v)` exactly is not feasible (taking time exponential in the dimension of de
:math:`h`). Therefore the most general approach is based on an *approximation*
of Monte-Carlo sampling called Markov chain Monte Carlo (MCMC).

A (first order) Markov chain is a sequence of random variables
:math:`Z_1,Z_2,\ldots`, where :math:`Z_k` is independent of
:math:`Z_{k-2}, Z_{k-3}, \ldots` given :math:`Z_{k-1}`:

.. math::
  P(Z_k | Z_{k-1}, Z_{k-2}, Z_{k-3}, \ldots) = P(Z_k | Z_{k-1})

  P(Z_1 \ldots Z_n) = P(Z_1) \prod_{k=2}^n P(Z_k|Z_{k-1})

The goal of MCMC is to construct a Markov chain whose asymptotic
marginal distribution, i.e. the distribution of :math:`Z_n` as
:math:`n \rightarrow \infty`, converges towards a given target
distribution, such as  :math:`P(h | v)` or :math:`P(x)`.

Gibbs sampling
--------------

Numerous MCMC-based sampling methods exist. The one most commonly
used for deep architectures is **Gibbs sampling**. It is simple and
presents a cer
Il existe de nombreuses méthodes d'échantillonage MCMC. Celle
la plus couramment utilisée pour les architectures
profondes est la **méthode d'échantillonage de Gibbs**
(*Gibbs sampling*). It is simple and has a certain plausible
analogy with the functioning of the brain, where each neuron
decides to send signals with a certain probability as a function
of the signals it receives from other neurons.

Let us suppose that we wish to sample from the distribution :math:`P(x)`
where :math:`x` is a set of variables :math:`x_i` (we could optionally
have a set of variables upon which we have conditioned, but this would
not change the procedure, so we ignore them in the following description).

Let :math:`x_{-i}=(x_1,x_2,\ldots,x_{i-1},x_{i+1},\ldots,x_n)`, i.e.
all variables in :math:`x` excluding :math:`x_i`. Gibbs sampling is
performed using the following algorithm:

* Choose an initial value of :math:`x` in an arbitrary manner (random or not)
* For each step of the Markov chain:

  * Iterate over each :math:`x_k` in :math:`x`

    * Draw :math:`x_k` from the conditional distribution :math:`P(x_k | x_{-k})`

In some cases one can group variables in :math:`x` into *blocks* or groups of
variables such that drawing samples for an entire group, given the others, is
easy. In this case it is advantageous ti interpret the algorithm above with
:math:`x_i` as the :math:`i^{\mathrm{th}}` group rather than the
:math:`i^{\mathrm{th}}` variable.  This is known as *block Gibbs sampling*.

The gradient in a log-linear Markov random field
------------------------------------------------

See
`Learning Deep Architectures for AI
<http://www.iro.umontreal.ca/%7Elisa/publications2/index.php/publications/show/239>`_
for detailed derivations.

Log-linear Markov random fields are undirected probabilistic models where
the energy function is *linear in the parameters* :math:`\theta` of the model:

.. math::
  P(x) \propto e^{-\sum_i \theta_i f_i(x)}

where :math:`f_i(.)` are known as *sufficient statistics*
of the model, because the expectations :math:`\mathbb{E}[f_i(x)]` are
sufficient for characterizing the distribution and estimating parameters.

Note that :math:`e^{\theta_i f_i(x)} = \psi_i(x)` is associated with
each clique in the model (in general, only a sub-vector of :math:`x`
influences each :math:`f_i(x)`).

Getting back to sufficient statistics, one can show that the
gradient of the log likelihood is as follows:

.. math::
  \frac{-\log P(x)}{\partial \theta_i} = f_i(x) - \sum_x P(x) f_i(x)

and the average gradient over training examples :math:`x_t` is thus

.. math::
  \frac{1}{T} \sum_t \frac{-\log P(x_t)}{\partial \theta_i} =
            \frac{1}{T}\sum_t f_i(x_t) - \sum_x P(x) f_i(x)

Thus, it is clear that the gradient vanishes when *the average
of the sufficient statistics under the training distribution
equals their expectation under the model distribution :math:`P`*.

Unfortunately, calculating this gradient is difficult. We do not want
to sum over all possible :math:`x`, but fortunately one can obtain
a Monte-Carlo approximation  by one or more draws from :math:`P(x)`,
which gives us a noisy estimate of the gradient. In general, however,
even to obtain an unbiased sample from :math:`P(x)` is exponentially
costly, and thus one must use an MCMC method.

We refer to the terms of the gradient due to the numerator of
the probability density (:math:`-f_i(x)`) as the **'positive phase'**
gradient, and the terms of the gradient corresponding to the partition
function (denominator of the probability density) as the
**'negative phase'** gradient.

Marginalization over hidden variables
-------------------------------------

When a model contains hidden variables, the gradient becomes
a bit more complicated since one must marginalize over the
hidden variables. Let :math:`x=(v,h)`, with :math:`v` being the visible
part and :math:`h` being the hidden part, with statistics
from functions of the two, :math:`f_i(v,h)`. The average gradient
of the negative log likelihood of the observed data is thus

.. math::
  \frac{1}{T} \sum_t \frac{-\log P(v_t)}{\partial \theta_i} =
            \frac{1}{T}\sum_t \sum_h P(h|v_t) f_i(v_t,h) - \sum_{h,v} P(v,h) f_i(v,h).

Il faudra donc dans ce cas généralement se résoudre à du MCMC
non seulement pour la partie négative mais aussi pour la partie
négative, pour échantillonner :math:`P(h|v_t)`.

The Boltzmann Machine
=====================

A Boltzmann machine is an undirected probabilistic model, a particular
form of log-linear *Markov random field*, containing both visible and hidden
variables, where the *energy function* is a *second degree polynomial* of
the variables :math:`x`:

.. math::
   E(x) = -d'x - x'Ax

The classic Boltzmann machine has binary variables and
inference is conducted via Gibbs sampling, which requires samples
from :math:`P(x_i | x_{-i})`. It can be easily shown that

.. math::
   P(x_i=1 | x_{-i}) = {\rm sigmoid}(d_i + \omega_i x_{-i})

where :math:`\omega_i` is the :math:`i^{th}` row of :math:`A` excluding
the :math:`i^{th}` element (the diagonal of :math:`A`
is 0 in this model). Thus, we see the link with networks of neurons.

.. _rbm:

Restricted Boltzmann Machines
-----------------------------

A *Restricted Boltzmann Machine*, or RBM, is a Boltzmann machine without
*lateral connections* between the visible units :math:`v_i` or between
the hidden units :math:`h_i`. The energy function thus becomes

.. math::
   E(v,h) = -b'h - c'v - v'W h.

where the matrix :math:`A` is entirely 0 except in the submatrix :math:`W`.
The advantage of this connectivity restriction is that inferring
:math:`P(h|v)` (and also :math:`P(v|h)`) becomes very easy, can be performed
analytically, and the distribution *factorizes*:

.. math::
   P(h|v) = \prod_i P(h_i|v)

and

.. math::
   P(v|h) = \prod_i P(v_i|h)

In the case where the variables("units") are binary, we obtain once again
a sigmoid activation probability:

.. math::
   P(h_j=1 | v) = {\rm sigmoid}(b_j + \sum_i W_{ij} v_i)

   P(v_i=1 | h) = {\rm sigmoid}(c_i + \sum_j W_{ij} h_j)

Another advantage of the RBM is that the distribution :math:`P(v)` can
be calculated analytically up to a constant (the unknown constant
being the partition function). This permits us to define a generalization
of the notion of an energy function in the case when we wish to marginalize
over the hidden variables: the **free energy** (inspired by notions from
physics)

.. math::
   P(v) = \frac{e^{-FE(v)}}{Z} = \sum_h P(v,h) = \frac{\sum_h e^{-E(v,h)}}{Z}

   FE(v) = -\log \sum_h e^{-E(v,h)}

and in the case of RBMs, we have

.. math::
   FE(v) = -b'v - \sum_i \log \sum_{h_i} e^{h_i (c_i + v' W_{.i})}

where the sum over :math:`h_i` is a sum over values that the hidden variables
can take, which in the case of binary units yields

.. math::
   FE(v) = -b'v - \sum_i \log (1 + e^{c_i + v' W_{.i}})

   FE(v) = -b'v - \sum_i {\rm softplus}(c_i + v' W_{.i})


Gibbs sampling in RBMs
----------------------

Although sampling from :math:`P(h|v)` is easy and immediate in an RBM,
drawing samples from :math:`P(v)` or from :math:`P(v,h)` cannot be done
exactly and is thus generally accomplished with MCMC, most commonly with
*block Gibbs sampling*, where we take advantage of the fact that
draws from :math:`P(h|v)` and :math:`P(v|h)` are easy:

.. math::
   v^{(1)} \sim {\rm exemple\;\; d'apprentissage}

   h^{(1)} \sim P(h | v^{(1)})

   v^{(2)} \sim P(v | h^{(1)})

   h^{(2)} \sim P(h | v^{(2)})

   v^{(3)} \sim P(v | h^{(2)})

   \ldots

In order to visualize the generated data at step :math:`k`, it is better
to use expectations (i.e. :math:`E[v^{(k)}_i|h^{(k-1)}]=P(v^{(k)}_i=1|h^{(k-1)})`)
which are less noisy than the samples :math:`v^{(k)}` themselves.

.. _trainrbm:

Training RBMs
=============

The exact gradient of the parameters of an RBM (for an example :math:`v`) is

.. math::
   \frac{\partial \log P(v)}{\partial W} = v' E[h | v] - E[v' h]

   \frac{\partial \log P(v)}{\partial b} = E[h | v] - E[h]

   \frac{\partial \log P(v)}{\partial c} = v - E[v]

where the expectations are under the distribution of the RBM. The conditional
expectations can be calculated analytically (since :math:`E[h_i | v]=P(h_i=1|v)=` the output of a hidden unit, for binary :math:`h_i`)
but the unconditional expectations must be approximated using MCMC.

Contrastive Divergence
----------------------

The first and simplest approximation of :math:`E[v' h]`, i.e., for
obtaining 'negative examples' (for the 'negative phase' gradient),
consists of running a short Gibbs chain (of :math:`k` steps) *beginning at
a training example*.  This algorithm is known as CD-k
(*Contrastive Divergence with k steps*). See algorithm 1
in `Learning Deep Architectures for AI <http://www.iro.umontreal.ca/%7Elisa/publications2/index.php/publications/show/239>`_:

.. math::
  W \leftarrow W + \epsilon( v^{(1)} \hat{h}^{(1)'} - v^{(2)} \hat{h}^{(2)'} )

  b \leftarrow b + \epsilon( \hat{h}^{(1)} - \hat{h}^{(2)} )

  c \leftarrow c + \epsilon( v^{(1)} - v^{(2)} )

où :math:`\epsilon` est le pas de gradient, et l'on réfère à la notation
de la chaîne de Gibbs des RBMs ci-haut, avec
:math:`\hat{h}^{(1)}` dénote le vecteur des probabilités :math:`P(h^{(1)}_i=1|v_1)`
et de la même manière :math:`\hat{h}^{(2)}_i=P(h^{(2)}_i=1|v^{(2)})`.

Ce qui est étonnant c'est que même avec k=1, on obtient
des RBMs qui fonctionnent bien dans le sens qu'elles extraient
des bonnes caractéristiques des entrées (ce qu'on peut vérifier
visuellement en regardant les filtres, les reconstructions
stochastiques après 1 étape de Gibbs, ou bien quantitativement
en initialisant chaque couche d'un réseau profond avec W et b par pré-entraînement
de la RBM associée à chaque couche).

On peut montrer que CD-1 est très proche de l'entraînement d'un auto-encodeur
par minimisation de l'erreur de reconstruction, et on voit que l'erreur
de reconstruction diminue de manière assez monotone pendant l'entraînement
par CD-1.

On peut aussi montrer que CD-k tends vers le vrai gradient (en espérance) quand
k devient grand, mais à ce moment on multiplie le temps de calcul par k.

Divergence Contrastive Persistente
----------------------------------

Pour obtenir un estimateur moins biaisé du vrai gradient sans augmenter
beaucoup le temps de calcul, on peut utiliser l'algorithme de Divergence
Contrastive Persistente (en anglais *Persistent Contrastive Divergence*,
ou PCD). Plutôt que de redémarrer une chaîne de Gibbs après avoir vu
chaque exemple :math:`v`, il s'agit de garder une chaîne de Gibbs toujours en activité
pour obtenir nos échantillons d'exemples négatifs. Cette chaîne est
un peu particulière car ses probabilités de transition changent
(lentement), au fur et à mesure qu'on met à jour les paramètres de
la RBM. Soit :math:`(v^-,h^-)` l'état de notre chaîne négative.
L'algorithme d'apprentissage est le suivant:

.. math::
  \hat{h}_i = P(h_i=1 | v)

  \forall i, \hat{v}^-_i = P(v_i=1 | h^-)

  v^- \sim \hat{v}^-

  \forall i, \widehat{h_i}^- = P(h_i=1 | v^-)

  h^- \sim \hat{h}^-

  W \leftarrow W + \epsilon( v \hat{h}' - v^- \hat{h}^{-'} )

  b \leftarrow b + \epsilon( \hat{h} - \hat{h}^- )

  c \leftarrow c + \epsilon( v - \hat{v}^- )


On trouve expérimentalement que PCD est meilleur en terme de génération
d'exemples (et en terme de vraisemblance :math:`\log P(v)`)
que CD-k, et est moins sensible à l'initialisation de la
chaîne de Gibbs.

.. _dbn:

RBMs empilés et DBNs
====================

On peut utiliser les RBMs comme les auto-encodeurs, pour pré-entraîner de manière
non-supervisée un réseau de neurones profonds, pour ensuite finaliser son entraînement
de manière supervisée habituelle. On va donc empiler les RBMs, la couche cachée
de l'un (étant donnée son entrée), i.e., les :math:`P(h|v)` ou bien des
:math:`h \sim P(h|v)`, devenant l'entrée de la couche suivante.

Le pseudo-code de l'entraînement *vorace* couche par couche d'une pile de RBMs
est présenté dans la section 6.1 (algorithme 2) de
`Learning Deep Architectures for AI <http://www.iro.umontreal.ca/%7Elisa/publications2/index.php/publications/show/239>`_.
Pour entraîner la k-ième RBM, on propage soit des échantillons
(:math:`h \sim P(h|v)`) ou des posterieurs (:math:`P(h|v)`) à travers
les k-1 premières RBMs, et on les utilise comme données d'entraînement
pour la k-ième RBM. On les entraîne donc une à la fois: une fois qu'on
arrête l'entraînement de la k-ième, on peut procéder à la k+1 ième.

Une RBM a la même paramétrisation qu'une couche classique de réseau de neurones
(avec des unités sigmoides), à la différence près qu'on utilise seulement les poids W
et les biais b des unités cachées (car on a seulement besoin de :math:`P(h|v)` et non pas
de :math:`P(v|h))`.

Deep Belief Networks
--------------------

On peut aussi considérer un empilement de RBMs de manière générative, et l'on
appelle ce modèle le Deep Belief Network:

.. math::

  P(x,h^1,\ldots,h^{\ell}) = \left( \prod_{k=0}^{\ell-2} P(h^k | h^{k+1}) \right) P(h^{\ell-1}, h^{\ell})

où l'on dénote :math:`x=h^0` et la variable (vecteur) aléatoire associée à la
couche k est :math:`h^k`. Les deux dernières couches ont une distribution jointe
qui est donnée par une RBM (la dernière de la pile). Les RBMs du dessous servent
seulement à définir les probabilités conditionnelles :math:`P(h^k | h^{k+1})`
du DBN, où :math:`h^k` joue le rôle d'unités visibles et :math:`h^{k+1}` joue
celui des unités cachées dans la RBM k+1.

Échantilloner d'un DBN se fait donc ainsi:

 * échantillonner un :math:`h^{\ell-1}` de la RBM du dessus (numéro :math:`\ell`), par exemple en faisant du Gibbs
 * pour k de :math:`\ell-1` à 1
    * échantillonner les unités visibles (:math:`h^k`) étant données les unités cachées
      (:math:`h^{k+1}`) dans la RBM k
 * retourner le dernier échantillon produit :math:`h^k`, qui est le résultat de la génération par le DBN

Dépliement d'une RBM et équivalence RBM - DBN
---------------------------------------------

On peut montrer (voir section 8.1 de
`Learning Deep Architectures for AI <http://www.iro.umontreal.ca/%7Elisa/publications2/index.php/publications/show/239>`_.)
qu'une RBM correspond à un DBN avec une architecture particulière, dont les poids
sont partagés entre toutes les couches: le niveau 1 du DBN utilise les poids W de la RBM,
le niveau 2 utilise les poids W', le niveau 3 utilise les poids W, etc. en alternant
entre W et W'. La dernière paire de couches du DBN est une RBM avec les poids W ou
W' selon qu'on déplie un nombre impair ou pair de couches.
Remarquons que dans cette équivalence, le DBN a des tailles de couches qui alternent
(nombre de visibles de la RBM, nombre de cachées de la RBM, nombre de visibles de la RBM, etc.).

En fait on peut continuer le dépliement d'une RBM jusqu'à l'infini et obtenir un réseau
dirigé infini à poids partagés, équivalent. Voir la figure 13 de la même section 8.1.
On peut voir que ce réseau infini correspond
exactement à une chaîne de Gibbs (infinie) qui culmine (finit) sur la couche visible
de la RBM originale, i.e., qui génère les mêmes exemples. Les couches paires correspondent
à échantillonner P(v|h) (de la RBM originale) et les couches impaires à échantillonner
P(h|v).

Finalement, on peut montrer que si on prend une RBM, qu'on la déplie une fois
(en miroir), le fait de continuer l'entraînement de la nouvelle RBM du dessus
(initialisée avec W') va maximiser une borne inférieure sur la vraisemblance
du DBN correspondant. Dans le passage d'une RBM à un DBN, on remplace
la marginale P(h) de la RBM (qui est obtenue implicitement à travers
les paramètres de la RBM, et par exemple une chaîne de Gibbs dans la RBM)
par la distribution générée par la partie de DBN au-dessus de cette RBM
(le DBN formé de toutes les couches au-dessus de h), puisque ce h correspond
aux unités visibles de ce DBN. La démo est simple et instructive, et
utilise la lettre Q pour les probabilités selon la RBM (du bas)
et la lettre P pour les probabilités selon la DBN obtenue en modélisant
les h différemment (i.e. en remplaçant Q(h) par P(h)). On remarque
aussi que P(x|h)=Q(x|h), mais ce n'est pas vrai pour P(h|x) et Q(h|x).

.. math::

  \log P(x) = \left(\sum_{h} Q(h|x)\right) \log P(x) = \sum_{h} Q(h|x) \log \frac{P(x,h)}{P(h|x)}

  \log P(x) = \sum_{h} Q(h|x) \log \frac{P(x,h)}{P(h|x)} \frac{Q(h|x)}{Q(h|x)}

  \log P(x) = H_{Q(h|x)} + \sum_{h} Q(h|x) \log P(x, h) + \sum_{h} Q(h|x) \log \frac{Q(h|x)}{P(h|x)}

  \log P(x) = KL(Q(h|x)||P(h|x)) + H_{Q(h|x)} + \sum_{h} Q(h|x) \left(\log P(h) + \log P(x|h) \right)

  \log P(x) \geq \sum_{h} Q(h|x) \left(\log P(h) + \log P(x|h) \right)

On voit donc effectivement que l'on peut augmenter la borne inférieure (dernière ligne)
en faisant de l'entraînement maximum de vraisemblance de P(h) utilisant comme données
d'entraînement des h tirés de Q(h|x), où x est tiré de la distribution d'entraînement
de la RBM du dessous. Étant donné qu'on a découplé les poids du dessous de ceux du
dessus, on ne touche pas à la RBM du dessous (P(x|h) et Q(h|x)), on modifie seulement
P(h).

Inférence approximative dans les DBNs
-------------------------------------

Contrairement à la RBM, l'inférence dans les DBNs (choisir les unités cachées
étant données les entrées visibles) est très difficile. Étant donné qu'on
initialise les DBNs comme une pile de RBMs, on utilise en pratique
l'approximation suivante: on échantillonne les :math:`h^k` étant
donné les :math:`h^{k-1}` en utilisant les poids du niveau k.
Il s'agirait de l'inférence exacte si c'était effectivement une
RBM isolée, mais ça ne l'est plus avec le DBN.

On a vu que c'est une approximation à la section précédente
parce que la marginale P(h) (du DBN) diffère de la marginale
Q(h) (de la RBM du dessous), après qu'on modifie les poids
du dessus qui ne sont plus la transposée des poids du
dessous, et donc P(h|x) diffère de Q(h|x).


Deep Boltzmann Machine
----------------------

Finalement, on peut aussi utiliser un empilement de RBMs pour initializer
une machine de Boltzmann profonde (Salakhutdinov et Hinton, AISTATS 2009).
Il s'agit d'une machine de Boltzmann organisée en couches, où chaque
couche est seulement connectée à celle du dessous et celle du dessus.

On remarque que les poids sont en quelque sorte deux fois trop gros
quand on fait cette initialisation, car maintenant chaque unité reçoit
son entrée de la couche au-dessus d'elle et aussi de la couche d'en dessous,
alors que dans la RBM originale c'était soit de l'un, ou de l'autre.
Salakhutdinov propose donc de diviser les poids par deux quand
on fait le passage de l'empilement de RBMs vers la machine de Boltzmann
profonde.

Il est intéressant de noter aussi que selon Salakhutdinov, il est crucial
de faire l'initialisation de la machine de Boltzmann profonde à partir de
l'empilement de RBMs, plutôt qu'à partir de poids aléatoires. Cela suggère
que la difficulté d'entraînement des réseaux MLP profonds déterministes
ne leur est pas unique, et qu'une difficulté semblable se retrouve dans
les machines de Boltzmann profondes. Dans les deux cas, le fait d'initialiser
chaque couche selon un entraînement local à la couche semble aider
beaucoup. Salakutdinov obtient des résultats meilleurs avec sa machine
de Boltzmann profonde qu'avec un DBN équivalent, mais l'entraînement
est plus long.















