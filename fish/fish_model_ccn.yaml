#
# JBY: from tutorial yaml
#

!obj:pylearn2.train.Train {
    dataset: &train_dataset !obj:fish.datasets.wiskott.WiskottVideo {
        which_set: 'train',   # train/valid/test just have different seeds
        quick: &enable_quick_mode True,

        config: &dataset_config !obj:fish.datasets.wiskott.WiskottVideoConfig {
            is_fish: True,
            num_frames: 3,
        }
    },

    #model: !obj:pylearn2.models.mlp.MLP {
    model: !obj:fish.fish_model.FishMLP {         # In Galatea repo
        batch_size: 6,   # Must be a multiple of num_frames!

        input_space: !obj:pylearn2.space.Conv2DSpace {
            shape: [156, 156],
            num_channels: 1,
            axes: ['c', 0, 1, 'b']
        },

        layers: [
            #!obj:pylearn2.models.mlp.ConvRectifiedLinear {
            #    layer_name: 'c0',     # (1,154,154) -> (16,50,60)
            #    kernel_shape: [5, 5],
            #    pool_shape: [3, 3],
            #    pool_stride: [3, 3],
            #    output_channels: 16,
            #    irange: .05,
            #    # Rather than using weight decay, we constrain the norms of the convolution kernels
            #    # to be at most this value
            #    max_kernel_norm: .9
            #},
            !obj:pylearn2.models.maxout.MaxoutConvC01B {
                layer_name: 'c0',     # (1,154,154) -> (16,50,60)
                tied_b: 1,
                W_lr_scale: .05,
                b_lr_scale: .05,
                num_channels: 16,
                num_pieces: 1,
                min_zero: True,
                kernel_shape: [5, 5],
                pool_shape: [3, 3],
                pool_stride: [3, 3],
                irange: .005,
                max_kernel_norm: .9,
                #partial_sum: 33,
            },

            #!obj:pylearn2.models.mlp.ConvRectifiedLinear {
            #    layer_name: 'c1',     # (16,50,60) -> (32,28,28)
            #    kernel_shape: [5, 5],
            #    pool_shape: [2, 2],
            #    pool_stride: [2, 2],
            #    output_channels: 32,
            #    irange: .05,
            #    # Rather than using weight decay, we constrain the norms of the convolution kernels
            #    # to be at most this value
            #    max_kernel_norm: .9
            #},
            !obj:pylearn2.models.maxout.MaxoutConvC01B {
                layer_name: 'c1',     # (16,50,60) -> (32,28,28)
                tied_b: 1,
                W_lr_scale: .05,
                b_lr_scale: .05,
                num_channels: 32,
                num_pieces: 1,
                min_zero: True,
                kernel_shape: [5, 5],
                pool_shape: [2, 2],
                pool_stride: [2, 2],
                irange: .005,
                max_kernel_norm: .9,
                #partial_sum: 33,
            },

            #!obj:pylearn2.models.mlp.ConvRectifiedLinear {
            #    layer_name: 'c2',     # (32,28,28) -> (32,8,8)
            #    kernel_shape: [5, 5],
            #    pool_shape: [3, 3],
            #    pool_stride: [3, 3],
            #    output_channels: 32,
            #    irange: .05,
            #    # Rather than using weight decay, we constrain the norms of the convolution kernels
            #    # to be at most this value
            #    max_kernel_norm: .9
            #},
            !obj:pylearn2.models.maxout.MaxoutConvC01B {
                layer_name: 'c2',     # (32,28,28) -> (32,8,8)
                tied_b: 1,
                W_lr_scale: .05,
                b_lr_scale: .05,
                num_channels: 32,
                num_pieces: 1,
                min_zero: True,
                kernel_shape: [5, 5],
                pool_shape: [3, 3],
                pool_stride: [3, 3],
                irange: .005,
                max_kernel_norm: .9,
                #partial_sum: 33,
            },

            !obj:pylearn2.models.mlp.Sigmoid {
                dim: 2048,
                layer_name: 'h0',
                irange: .5,
            },

            !obj:pylearn2.models.mlp.CompositeLayer {
                layer_name: 'hout',
                layers: [
                    !obj:pylearn2.models.mlp.Softmax {
                        layer_name: 'hout_id',
                        max_col_norm: 1.0,
                        n_classes: 25,
                        irange: .005
                    },
                    !obj:pylearn2.models.mlp.Sigmoid {
                        layer_name: 'hout_xysincos',
                        dim: 4,
                        irange: .5,
                    },
                ]
            },

        ],
    },

    algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {
        learning_rate : .5,

        learning_rule: !obj:pylearn2.training_algorithms.learning_rule.Momentum {
            init_momentum: .9,
        },

        # Each gradient step will be based on this many examples
        #batch_size : 48,

        # Batches in one epoch
        batches_per_iter : 4,

        monitoring_batches : 2,

        monitoring_dataset : {
            'train': *train_dataset,
            'valid': &valid_dataset !obj:fish.datasets.wiskott.WiskottVideo {
                which_set: 'valid',
                config: *dataset_config,   # Make sure config is the same as above
                quick: *enable_quick_mode
            }
        },

        monitor_iteration_mode : 'shuffled_sequential',  # ...currently ignored anyway...

        #cost : !obj:pylearn2.costs.cost_pred_net.PredNetCostMLP {
        #},

        # We'll use the monitoring dataset to figure out when to stop training.
        #
        # In this case, we stop if there is less than a 1% decrease in the
        # training error in the last epoch.  You'll notice that the learned
        # features are a bit noisy. If you'd like nice smooth features you can
        # make this criterion stricter so that the model will train for longer.
        # (setting N to 10 should make the weights prettier, but will make it
        # run a lot longer)

        termination_criterion : !obj:pylearn2.termination_criteria.EpochCounter {
            max_epochs: 5,
        },

    },

    save_freq : 50
}
